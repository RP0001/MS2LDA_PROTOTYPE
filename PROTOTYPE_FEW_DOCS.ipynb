{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDIA_ROOT is C:\\Users\\rpetr\\OneDrive\\Desktop\\DISS_CODE\\ms2ldaviz\\ms2ldaviz\\media\n"
     ]
    }
   ],
   "source": [
    "# all the relevant imports are done here\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "basedir = 'C:\\\\Users\\\\rpetr\\\\OneDrive\\\\Desktop\\\\DISS_CODE\\\\ms2ldaviz\\\\ms2ldaviz'\n",
    "sys.path.append(basedir)\n",
    "import django\n",
    "import json\n",
    "django.setup()\n",
    "from basicviz.models import Experiment, Alpha, Mass2MotifInstance, FeatureInstance, Feature, Document, Mass2Motif, DocumentMass2Motif, FeatureMass2MotifInstance\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import csv\n",
    "from scipy.special import polygamma as pg\n",
    "from scipy.special import psi as psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Choose an experiment. In this case it is experiment 190. It has 500 topics, 27923 words and 2132 unique docs. These were tested against the database using appropriate queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id=190 \n",
    "experiment = Experiment.objects.get(id=experiment_id)\n",
    "min_prob_beta = 1e-3\n",
    "SMALL_NUMBER = 1e-100\n",
    "eta = 0.1 #needed for beta m-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORPUS (features for 1 document in experiment 190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First we get all features in the database for our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all features in the database relevant for our experiment. \n",
    "features = Feature.objects.filter(experiment_id=experiment)\n",
    "experiment_words = []\n",
    "for f in features:\n",
    "     if f.id not in experiment_words: \n",
    "        experiment_words.append(f.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique words lists all the features as {feature_id:incremental_id} word value pairs. \n",
    "unique_words = {}\n",
    "index = 0\n",
    "for word in experiment_words:\n",
    "    if word not in unique_words.keys():\n",
    "        unique_words.update({word:index})\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Then we get a random document for our experiment from the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a single document for our experiment. Modify here if more documents are needed. \n",
    "experiment_docs=[270414]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique documents is the dictionary -> doc: id \n",
    "unique_docs = {}\n",
    "index = 0 \n",
    "for doc in experiment_docs: \n",
    "    unique_docs.update({doc:index})\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the features only for the specific documents and create the corpus dictionary {DOC:{WORD:COUNT}}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for all documents chosen. The output columns are doc_id, word_id and intensity.\n",
    "feature_instances = FeatureInstance.objects.filter(document_id__in=unique_docs.keys(), feature_id__in=unique_words.keys())\n",
    "doc_word_data = []\n",
    "for f in feature_instances:\n",
    "    doc_word_data.append([unique_docs[int(f.document_id)], unique_words[int(f.feature_id)], f.intensity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a csv for the corpus in order to create a dictionary made up of {document_id:{word_id:intensity}}. \n",
    "# Intensity in this case is an integer (count).\n",
    "doc_word_array = np.array(doc_word_data)\n",
    "np.savetxt(\"corpus_data.csv\", doc_word_array, delimiter=\",\", fmt=\"%s\")\n",
    "np.save(\"corpus_data\",doc_word_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATE THE CORPUS - a dictionary where key is document id and value is a dict of the count of words\n",
    "# You might need to change the csv output above (for now, by adding an extra row with doc_id, word_id, count)\n",
    "corpus_dict = {}\n",
    "with open(\"corpus_data.csv\", 'r') as data_file:\n",
    "    data = csv.DictReader(data_file, delimiter=\",\")\n",
    "    for row in data:\n",
    "        item = corpus_dict.get(row[\"doc_id\"], dict())\n",
    "        item[row[\"word_id\"]] = int(row[\"count\"])\n",
    "        corpus_dict[row[\"doc_id\"]] = item\n",
    "#Get the corpus dict whenever this is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIQUE TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 500 unique topics for the experiment. \n",
    "mi = Mass2Motif.objects.filter(experiment=experiment)\n",
    "unique_topics = {}\n",
    "index=0\n",
    "for m in mi: \n",
    "    unique_topics.update({m.id:index})\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the alphas from the database (it is a vector)\n",
    "al = Alpha.objects.filter(mass2motif__experiment=experiment).order_by('mass2motif')\n",
    "alphas = {}\n",
    "for a in al:\n",
    "    alphas.update({unique_topics[a.mass2motif_id]: a.value})\n",
    "n_motif = len(alphas)\n",
    "alpha_vec = np.zeros(n_motif)\n",
    "for pos,val in alphas.items():\n",
    "    alpha_vec[pos] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vector = alpha_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to text if necessary \n",
    "# np.savetxt(\"alpha.csv\", alpha_vector, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get beta from the database (it is a topic * words 2d matrix - each cell is a probability)\n",
    "beta_pre_pivot = []\n",
    "mi = Mass2MotifInstance.objects.filter(mass2motif__experiment=experiment)\n",
    "for m in mi:\n",
    "    beta_pre_pivot.append([unique_topics[m.mass2motif_id], unique_words[m.feature_id], m.probability]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some topics may have 0 words - these have been reincluded \n",
    "# Creating array from the beta data and subsequently creating a pivot (matrix)\n",
    "output_arr_beta = np.array(beta_pre_pivot)\n",
    "K = len(unique_topics)\n",
    "W = len(unique_words)\n",
    "pivot_table = np.zeros((K, W)).astype('float')\n",
    "i = 0\n",
    "max = len(beta_pre_pivot)\n",
    "while i<max:\n",
    "    pivot_table[int(output_arr_beta[i][0]),int(output_arr_beta[i][1])]=output_arr_beta[i][2]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to get beta csv. \n",
    "# np.savetxt(\"beta.csv\", pivot_table, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the beta pivot table matrix. \n",
    "pivot_table_normalised = pivot_table\n",
    "i = 0\n",
    "while i<K: \n",
    "    row = pivot_table_normalised[i, :]\n",
    "    adjusted_row = row + SMALL_NUMBER\n",
    "    normalised_row = adjusted_row / np.sum(adjusted_row)\n",
    "    np.sum(normalised_row)\n",
    "    pivot_table_normalised[i, :] = normalised_row\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to output a csv for the beta matrix if needed. \n",
    "# np.savetxt(\"beta_matrix.csv\", pivot_table_normalised, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for visualisation if necessary \n",
    "# my_dpi=150\n",
    "# plt.figure(figsize=(2000/my_dpi, 2000/my_dpi), dpi=my_dpi)\n",
    "# plt.imshow(pivot_table_normalised, aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ORIGINAL THETA(NORM GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the original theta from the database for subsequent comparison \n",
    "# remember theta is just normalised gamma and represents a docs * topics matrix \n",
    "theta = DocumentMass2Motif.objects.filter(document_id__in=experiment_docs)\n",
    "output_data_theta = []\n",
    "for t in theta:\n",
    "    output_data_theta.append([unique_docs[int(t.document_id)], unique_topics[int(t.mass2motif_id)], t.probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 271, 0.925464030966764],\n",
       " [0, 413, 0.0418494260642831],\n",
       " [0, 468, 0.0101604220097222],\n",
       " [0, 200, 0.0159164772206027]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ORIGINAL PHI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the features related to the instances\n",
    "feature_instance = FeatureInstance.objects.filter(document_id__in=experiment_docs)\n",
    "feature_instance_join = {}\n",
    "for i in feature_instance:\n",
    "    feature_instance_join.update({int(i.id):[int(i.document_id), int(unique_words[i.feature_id])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect docs, topics and features into a list of lists\n",
    "feature_m2m_instance = FeatureMass2MotifInstance.objects.filter(mass2motif__experiment=experiment)\n",
    "phi_list = []\n",
    "for i in feature_m2m_instance:\n",
    "    if i.featureinstance_id in feature_instance_join.keys():\n",
    "        phi_list.append([feature_instance_join[int(i.featureinstance_id)][0], unique_topics[int(i.mass2motif_id)], feature_instance_join[int(i.featureinstance_id)][1],i.probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives the original phi, which in abstract terms is a 3D matrix -> docs * topics * words\n",
    "phi_original = []\n",
    "for line in phi_list: \n",
    "    phi_original.append([line[0],line[2],line[1],line[3]])\n",
    "phi_original_array = np.array(phi_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"phi_original_array.csv\", phi_original_array, delimiter=\",\", fmt=\"%s\")\n",
    "# np.save(\"phi_original_array\", phi_original_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-STEP (has 9 steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - E-step variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_vector is already mentioned above\n",
    "# beta_matrix is created here from pivot_table_normalised\n",
    "# K and W are from above for total unique topics, total unique words respectively \n",
    "# you need a corpus (created above)\n",
    "corpus = corpus_dict\n",
    "beta_matrix = pivot_table_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Initialise phi matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the 3D matrix phi with zeroes\n",
    "phi_matrix={}\n",
    "for doc in corpus: \n",
    "    d = int(doc)\n",
    "    phi_matrix[d] = {}\n",
    "    for word in corpus[doc]:\n",
    "        w = int(word)\n",
    "        phi_matrix[d][w]=np.zeros(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - initialise gamma matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gamma matrix with rows as documents and columns as topics \n",
    "# this will later be transposed in order to create the phi matrix in the steps 3-9 below\n",
    "# doc_total is the count of words per doc, and each gamma is alpha plus that\n",
    "\n",
    "gamma_matrix=np.zeros((int(len(corpus)),int(K))) #3x500 shape\n",
    "for doc in corpus:\n",
    "    doc_total=0.0\n",
    "    for word in corpus[doc]:\n",
    "        doc_total += corpus[doc][word]\n",
    "    gamma_matrix[int(doc),:] = alpha_vector + 1.0*(doc_total/K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - 9: repeat until convergence loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise phi and do Blei's loop (Simon's implementation)\n",
    "test_list = []\n",
    "iterations=10000\n",
    "n_words = int(len(unique_words))\n",
    "temp_beta = np.zeros((K, n_words))\n",
    "current_gamma = np.copy(gamma_matrix)\n",
    "for i in range(iterations):   \n",
    "    prev_gamma = np.copy(current_gamma)\n",
    "    for doc in corpus:\n",
    "        d = int(doc)\n",
    "        doc_dict = corpus[doc]\n",
    "        temp_gamma = np.zeros(K) + alpha_vector\n",
    "        for word in doc_dict: #the word is actually column positioning so we do not need n^3 complexity \n",
    "            w = int(word)\n",
    "            log_phi_matrix = np.log(beta_matrix[:,w]) + psi(gamma_matrix[d,:]).T\n",
    "            log_phi_matrix = np.exp(log_phi_matrix - log_phi_matrix.max())\n",
    "            phi_matrix[d][w] = log_phi_matrix/log_phi_matrix.sum()\n",
    "            temp_gamma += phi_matrix[d][w]*corpus[doc][word]\n",
    "            temp_beta[:,w] += phi_matrix[d][w] * corpus[doc][word]\n",
    "        gamma_matrix[d,:] = temp_gamma\n",
    "        pos = np.where(gamma_matrix[d,:]<SMALL_NUMBER)[0]\n",
    "        gamma_matrix[d,pos] = SMALL_NUMBER\n",
    "    current_gamma = np.copy(gamma_matrix)\n",
    "    gamma_diff = ((current_gamma - prev_gamma)**2).sum()\n",
    "#     beta_matrix = temp_beta\n",
    "    test_list.append([i, gamma_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100, 3.5193330180069845e-14],\n",
       " [101, 2.636396940921179e-14],\n",
       " [102, 1.976042693444582e-14],\n",
       " [103, 1.4818950231490434e-14],\n",
       " [104, 1.1118956342593135e-14],\n",
       " [105, 8.347252707949417e-15],\n",
       " [106, 6.269711106110499e-15],\n",
       " [107, 4.711608502277131e-15],\n",
       " [108, 3.5425603208161425e-15],\n",
       " [109, 2.6648135009429643e-15]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the output list of the above e-step \n",
    "test_list[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array output if necessary \n",
    "# gamma_diff_array = np.array(test_list)\n",
    "# np.savetxt(\"gamma_diff_array.csv\", gamma_diff_array, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON OF GAMMA & PHI (original vs calculated csv exports) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma comparison / actually Theta comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_data_theta is the original data we work with, which is an 3 column matrix (doc_id, topic_id, probability)\n",
    "# we aim to transform output_data_theta into a normalised vector \n",
    "# note that this implementation only works for a single document \n",
    "gamma_vector_original = np.zeros(K) \n",
    "for line in range(len(output_data_theta)):\n",
    "    pos = int(output_data_theta[line][1])\n",
    "    prob = output_data_theta[line][2]\n",
    "    gamma_vector_original[pos] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the original gamma vector\n",
    "gamma_vector_original += SMALL_NUMBER\n",
    "gamma_vector_original /= np.sum(gamma_vector_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.75880630e-104, 1.56862745e-104, 2.04819575e-104, 1.56862745e-104,\n",
       "       2.08845699e-104, 2.10658272e-104, 1.37608627e-003, 2.13982185e-104,\n",
       "       2.11189716e-003, 2.02543919e-104, 1.75881052e-104, 2.19751666e-104,\n",
       "       2.16989820e-104, 1.84631812e-104, 1.84631811e-104, 1.56862745e-104,\n",
       "       1.56862745e-104, 2.06908157e-104, 1.84631825e-104, 1.89917345e-104,\n",
       "       2.37689068e-104, 2.18398058e-104, 1.75938096e-104, 2.18398058e-104,\n",
       "       1.56862745e-104, 1.84631813e-104, 1.97195111e-104, 1.75881912e-104,\n",
       "       1.56862745e-104, 2.33128600e-104, 2.04819575e-104, 4.73571838e-003,\n",
       "       2.77364013e-003, 1.56862745e-104, 1.84631827e-104, 1.75881286e-104,\n",
       "       2.02543919e-104, 2.00683406e-003, 1.93908598e-104, 1.56862745e-104,\n",
       "       1.84631812e-104, 1.24372380e-003, 2.06908157e-104, 2.00029086e-104,\n",
       "       1.93908598e-104, 1.56862745e-104, 2.34359472e-003, 2.31570761e-003,\n",
       "       2.24717263e-104, 2.04819575e-104, 1.56862745e-104, 2.04819575e-104,\n",
       "       2.02543919e-104, 5.40645595e-004, 2.61521034e-104, 2.21056088e-104,\n",
       "       2.13982185e-104, 5.92589714e-004, 1.93908598e-104, 2.73495596e-003,\n",
       "       1.89917345e-104, 1.89952785e-104, 1.89917345e-104, 2.00029086e-104,\n",
       "       1.56862745e-104, 1.84631811e-104, 2.06908157e-104, 2.02543919e-104,\n",
       "       2.12365414e-104, 1.89917345e-104, 1.56862745e-104, 1.49349904e-003,\n",
       "       2.56045255e-104, 1.89917345e-104, 1.89917345e-104, 1.56862745e-104,\n",
       "       2.13982185e-104, 2.94850627e-003, 2.10658272e-104, 5.34306520e-004,\n",
       "       2.10658272e-104, 1.84631828e-104, 1.56862745e-104, 2.02543919e-104,\n",
       "       2.06908157e-104, 2.22315944e-104, 4.15494846e-004, 2.36810551e-104,\n",
       "       2.10658272e-104, 1.84631815e-104, 2.04819575e-104, 2.06908157e-104,\n",
       "       2.51451217e-104, 1.93908598e-104, 1.93908598e-104, 1.97195111e-104,\n",
       "       1.75881430e-104, 7.87311145e-004, 1.75880620e-104, 2.10658272e-104,\n",
       "       1.56862745e-104, 1.97195111e-104, 1.89917345e-104, 2.08845699e-104,\n",
       "       1.97195111e-104, 1.84631813e-104, 6.26102732e-004, 1.89917345e-104,\n",
       "       1.56862745e-104, 1.89917345e-104, 1.56862745e-104, 1.84631829e-104,\n",
       "       2.54116427e-104, 1.93908598e-104, 1.89917345e-104, 1.47208029e-003,\n",
       "       2.10658272e-104, 5.47733727e-004, 1.64381493e-003, 5.77726010e-004,\n",
       "       1.84631812e-104, 1.84631814e-104, 2.10658272e-104, 2.12365414e-104,\n",
       "       2.06908157e-104, 1.97195111e-104, 2.00029086e-104, 1.89917345e-104,\n",
       "       2.20525680e-003, 5.11290056e-004, 1.56862745e-104, 2.00029086e-104,\n",
       "       2.00029086e-104, 1.84631811e-104, 2.50074743e-104, 1.84631814e-104,\n",
       "       5.76514256e-004, 2.04819575e-104, 2.41226296e-003, 1.84631812e-104,\n",
       "       2.06908157e-104, 2.15520467e-104, 1.75952642e-104, 1.75880721e-104,\n",
       "       2.00029086e-104, 1.84631812e-104, 1.89917345e-104, 2.33128600e-104,\n",
       "       1.56862745e-104, 1.84631811e-104, 2.33128600e-104, 2.41058756e-104,\n",
       "       2.86930734e-003, 1.56862745e-104, 2.25865129e-104, 1.93908598e-104,\n",
       "       2.15520467e-104, 2.40236660e-104, 1.89917345e-104, 8.79676857e-002,\n",
       "       1.75882868e-104, 1.56862745e-104, 7.45918144e-004, 1.84631818e-104,\n",
       "       2.08845699e-104, 1.93908598e-104, 2.08845699e-104, 1.75880628e-104,\n",
       "       2.04819575e-104, 1.84631811e-104, 1.89917345e-104, 2.15520467e-104,\n",
       "       1.56862745e-104, 2.30162495e-104, 1.75880638e-104, 2.08845699e-104,\n",
       "       7.27059887e-004, 1.75881165e-104, 2.27520690e-003, 4.96441936e-004,\n",
       "       1.75880636e-104, 1.84631812e-104, 5.01665589e-004, 1.56862745e-104,\n",
       "       1.84631821e-104, 1.89917345e-104, 1.56862745e-104, 2.00029086e-104,\n",
       "       1.84631811e-104, 1.97195111e-104, 1.95570657e-003, 1.89917345e-104,\n",
       "       4.84506052e-004, 1.56862745e-104, 1.93908598e-104, 2.13982185e-104,\n",
       "       4.71519957e-004, 1.56862745e-104, 1.66152924e-004, 1.97195111e-104,\n",
       "       1.82902038e-002, 1.30096860e-003, 2.04819575e-104, 2.02543919e-104,\n",
       "       1.93908598e-104, 2.02543919e-104, 2.00029086e-104, 1.89917345e-104,\n",
       "       1.18959374e-003, 1.97195111e-104, 3.65517284e-004, 2.12365414e-104,\n",
       "       2.02543919e-104, 2.21056088e-104, 2.02543919e-104, 2.10658272e-104,\n",
       "       6.55648773e-004, 1.97195111e-104, 1.56862745e-104, 1.84631811e-104,\n",
       "       2.02543919e-104, 1.97195111e-104, 1.84631813e-104, 1.97195111e-104,\n",
       "       1.02010551e-003, 2.32161317e-104, 2.13982185e-104, 2.33128600e-104,\n",
       "       1.97195111e-104, 1.93908598e-104, 4.69278660e-004, 2.06908157e-104,\n",
       "       1.75884506e-104, 1.97195111e-104, 2.06908157e-104, 3.04970468e-003,\n",
       "       3.86524995e-003, 2.69289396e-104, 1.56862745e-104, 1.93908598e-104,\n",
       "       1.15238152e-003, 1.84631812e-104, 1.97195111e-104, 1.84631813e-104,\n",
       "       1.56862745e-104, 2.15520467e-104, 5.16605705e-003, 2.70781317e-003,\n",
       "       1.89917345e-104, 1.93908598e-104, 1.89917345e-104, 2.04819575e-104,\n",
       "       1.60433526e-003, 2.06908157e-104, 1.89917345e-104, 1.93908598e-104,\n",
       "       1.56862745e-104, 2.46489270e-104, 1.97195111e-104, 2.04819575e-104,\n",
       "       1.84631812e-104, 3.22172560e-003, 8.75478580e-004, 2.02543919e-104,\n",
       "       2.19751702e-104, 2.02543919e-104, 2.08845699e-104, 1.93908598e-104,\n",
       "       2.12365414e-104, 2.06908157e-104, 1.56862745e-104, 7.02023679e-001,\n",
       "       1.75880772e-104, 1.37831083e-003, 1.97195111e-104, 2.59743540e-104,\n",
       "       1.84631812e-104, 2.16989820e-104, 9.41899167e-004, 1.56862745e-104,\n",
       "       2.08845699e-104, 1.56862745e-104, 1.75880639e-104, 1.79024765e-003,\n",
       "       2.57366235e-003, 1.89917345e-104, 2.44990980e-104, 1.89917345e-104,\n",
       "       1.56862745e-104, 1.41334883e-003, 1.93908598e-104, 1.84631818e-104,\n",
       "       1.84633537e-104, 1.84631812e-104, 1.75880698e-104, 3.23401953e-004,\n",
       "       1.89917345e-104, 2.02543919e-104, 2.23798714e-003, 1.75880678e-104,\n",
       "       1.93908598e-104, 1.84631813e-104, 1.75889220e-104, 1.75883323e-104,\n",
       "       1.56862745e-104, 1.56862745e-104, 1.56862745e-104, 1.56862745e-104,\n",
       "       1.84631812e-104, 2.00029086e-104, 1.89917345e-104, 2.10658272e-104,\n",
       "       1.56862745e-104, 1.84631811e-104, 2.02543919e-104, 3.64220402e-004,\n",
       "       9.89678775e-004, 1.89917345e-104, 2.12365414e-104, 2.00029086e-104,\n",
       "       1.97195111e-104, 2.30162495e-104, 1.82026052e-004, 9.82412869e-004,\n",
       "       1.75880702e-104, 1.96472237e-003, 1.93908598e-104, 2.02543919e-104,\n",
       "       2.00029086e-104, 2.02543919e-104, 1.15302826e-003, 1.56862745e-104,\n",
       "       5.30978982e-003, 1.93908598e-104, 7.80962881e-004, 1.84631812e-104,\n",
       "       1.93908598e-104, 1.63944054e-003, 2.02543919e-104, 9.71205883e-004,\n",
       "       1.97195111e-104, 1.84631830e-104, 2.06908157e-104, 1.84631812e-104,\n",
       "       1.93908598e-104, 1.93908598e-104, 2.25865129e-104, 1.89917345e-104,\n",
       "       1.84631812e-104, 2.13982185e-104, 1.75880671e-104, 1.97195111e-104,\n",
       "       1.97195111e-104, 1.56862745e-104, 2.88132063e-004, 1.56862745e-104,\n",
       "       1.93908598e-104, 6.09807102e-003, 1.29091909e-003, 2.02543919e-104,\n",
       "       1.75881606e-104, 2.45729577e-003, 2.10658272e-104, 1.56862745e-104,\n",
       "       2.06908157e-104, 1.75890612e-104, 2.35916128e-104, 1.54052397e-003,\n",
       "       2.08845699e-104, 2.08845699e-104, 1.56862745e-104, 1.33166215e-003,\n",
       "       3.37242670e-104, 2.19751666e-104, 4.52566068e-004, 1.97195111e-104,\n",
       "       1.18964278e-003, 2.12365414e-104, 2.24717263e-104, 3.37478079e-004,\n",
       "       1.75880987e-104, 1.89917345e-104, 2.19751666e-104, 1.56862745e-104,\n",
       "       6.29542013e-004, 1.89917345e-104, 2.02543919e-104, 1.84631811e-104,\n",
       "       1.93908598e-104, 2.16989820e-104, 1.75889016e-104, 3.11179622e-003,\n",
       "       1.25748493e-003, 2.04819575e-104, 1.89917345e-104, 7.47769222e-004,\n",
       "       2.18398058e-104, 2.00029086e-104, 1.36799838e-003, 1.89917345e-104,\n",
       "       1.89917345e-104, 2.15520467e-104, 2.82887544e-104, 1.56862745e-104,\n",
       "       2.04819575e-104, 1.97195111e-104, 2.16989820e-104, 1.84631811e-104,\n",
       "       2.10658272e-104, 2.06908157e-104, 2.08845699e-104, 2.80610022e-104,\n",
       "       2.37689068e-104, 4.18791995e-002, 3.09274979e-104, 2.35004955e-104,\n",
       "       3.05356618e-104, 2.71372171e-104, 2.32161317e-104, 2.02543919e-104,\n",
       "       1.75880770e-104, 2.39401432e-104, 2.04819575e-104, 1.93908598e-104,\n",
       "       1.75880665e-104, 2.29128153e-104, 2.05244971e-003, 2.35004955e-104,\n",
       "       3.05356618e-104, 2.88137111e-104, 2.88561901e-104, 7.10926952e-004,\n",
       "       3.09584768e-004, 2.80147572e-104, 1.01866405e-003, 7.17241869e-004,\n",
       "       1.75880650e-104, 2.24717263e-104, 2.02543919e-104, 2.34076113e-104,\n",
       "       2.04819575e-104, 1.56862745e-104, 1.64656490e-004, 2.04819575e-104,\n",
       "       2.41868292e-104, 1.93908598e-104, 3.99142038e-004, 2.16989820e-104,\n",
       "       2.40236660e-104, 1.93908598e-104, 1.56862745e-104, 1.56862745e-104,\n",
       "       1.89918375e-104, 2.24717263e-104, 1.56862745e-104, 2.02543919e-104,\n",
       "       1.47102996e-003, 2.55408602e-104, 2.36810551e-104, 2.06908157e-104,\n",
       "       1.93908598e-104, 1.56862745e-104, 1.75880742e-104, 1.93908598e-104,\n",
       "       1.97195111e-104, 1.93908598e-104, 3.12046416e-104, 2.80610022e-104,\n",
       "       1.01929065e-002, 2.44226695e-104, 2.52128144e-104, 2.50655526e-003,\n",
       "       2.39401432e-104, 2.95521417e-104, 2.25865129e-104, 3.22561455e-104,\n",
       "       2.49167418e-004, 2.43451780e-104, 2.33128600e-104, 1.33166215e-003,\n",
       "       2.53460535e-104, 2.63824360e-104, 2.00029086e-104, 2.35004955e-104,\n",
       "       1.84631812e-104, 1.03809793e-003, 2.38552456e-104, 2.08845699e-104,\n",
       "       2.00029086e-104, 2.16989820e-104, 2.13982185e-104, 2.13982185e-104,\n",
       "       2.13982185e-104, 9.67770037e-004, 2.16989820e-104, 2.29128153e-104,\n",
       "       2.39401432e-104, 2.52128144e-104, 2.81983342e-104, 2.74751267e-003])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalise calculated gamma vector\n",
    "gamma_vector_calculated = np.zeros(K) \n",
    "gamma_vector_calculated = np.copy(gamma_matrix[0])\n",
    "gamma_vector_calculated /= np.sum(gamma_vector_calculated)\n",
    "gamma_vector_calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for comparison - to be automatised later \n",
    "# np.savetxt(\"compare_gamma1.csv\", gamma_vector_original, delimiter=\",\", fmt=\"%s\")\n",
    "# np.savetxt(\"compare_gamma2.csv\", gamma_vector_calculated, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the array form of the original phi to csv.\n",
    "# np.savetxt(\"phi_original_array.csv\", phi_original_array, delimiter=\",\", fmt=\"%s\")\n",
    "# np.save(\"phi_original_array\", phi_original_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bring Phi Matrix calculated in e-step to list form here. \n",
    "phi_calculated = []\n",
    "for line in phi_list: \n",
    "    line_doc = unique_docs[line[0]]\n",
    "    line_topic = int(line[2])\n",
    "    line_word = int(line[1])\n",
    "    line_prob = phi_matrix[line_doc][line_topic][line_word]\n",
    "    phi_calculated.append([line_doc, line_topic, line_word, line_prob])\n",
    "phi_calculated_array = np.array(phi_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the array form of the new phi to csv.\n",
    "np.savetxt(\"phi_calculated_array.csv\", phi_calculated_array, delimiter=\",\", fmt=\"%s\")          \n",
    "np.save(\"phi_calculated_array\", phi_calculated_array)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATED COMPARISON GAMMA/PHI (to be completed) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
