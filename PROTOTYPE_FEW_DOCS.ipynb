{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDIA_ROOT is C:\\Users\\rpetr\\OneDrive\\Desktop\\DISS_CODE\\ms2ldaviz\\ms2ldaviz\\media\n"
     ]
    }
   ],
   "source": [
    "# all the relevant imports are done here\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "basedir = 'C:\\\\Users\\\\rpetr\\\\OneDrive\\\\Desktop\\\\DISS_CODE\\\\ms2ldaviz\\\\ms2ldaviz'\n",
    "sys.path.append(basedir)\n",
    "import django\n",
    "import json\n",
    "django.setup()\n",
    "from basicviz.models import Experiment, Alpha, Mass2MotifInstance, FeatureInstance, Feature, Document, Mass2Motif, DocumentMass2Motif, FeatureMass2MotifInstance\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import csv\n",
    "from scipy.special import polygamma as pg\n",
    "from scipy.special import psi as psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Choose an experiment. In this case it is experiment 190. It has 500 topics, 27923 words and 2132 unique docs. These were tested against the database using appropriate queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id=190 \n",
    "experiment = Experiment.objects.get(id=experiment_id)\n",
    "min_prob_beta = 1e-3\n",
    "SMALL_NUMBER = 1e-100\n",
    "eta = 0.1 #needed for beta m-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORPUS (features for 1 document in experiment 190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First we get all features in the database for our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all features in the database relevant for our experiment. \n",
    "features = Feature.objects.filter(experiment_id=experiment)\n",
    "experiment_words = []\n",
    "for f in features:\n",
    "     if f.id not in experiment_words: \n",
    "        experiment_words.append(f.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique words lists all the features as {feature_id:incremental_id} word value pairs. \n",
    "unique_words = {}\n",
    "index = 0\n",
    "for word in experiment_words:\n",
    "    if word not in unique_words.keys():\n",
    "        unique_words.update({word:index})\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Then we get a random document for our experiment from the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a single document for our experiment. Modify here if more documents are needed. \n",
    "experiment_docs=[269323]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique documents is the dictionary -> doc: id \n",
    "unique_docs = {}\n",
    "index = 0 \n",
    "for doc in experiment_docs: \n",
    "    unique_docs.update({doc:index})\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the features only for the specific documents and create the corpus dictionary {DOC:{WORD:COUNT}}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for all documents chosen. The output columns are doc_id, word_id and intensity.\n",
    "feature_instances = FeatureInstance.objects.filter(document_id__in=unique_docs.keys(), feature_id__in=unique_words.keys())\n",
    "doc_word_data = []\n",
    "for f in feature_instances:\n",
    "    doc_word_data.append([unique_docs[int(f.document_id)], unique_words[int(f.feature_id)], f.intensity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a csv for the corpus in order to create a dictionary made up of {document_id:{word_id:intensity}}. \n",
    "# Intensity in this case is an integer (count).\n",
    "doc_word_array = np.array(doc_word_data)\n",
    "np.savetxt(\"corpus_data.csv\", doc_word_array, delimiter=\",\", fmt=\"%s\")\n",
    "np.save(\"corpus_data\",doc_word_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CREATE THE CORPUS - a dictionary where key is document id and value is a dict of the count of words\n",
    "corpus_dict = {}\n",
    "with open(\"corpus_data.csv\", 'r') as data_file:\n",
    "    data = csv.DictReader(data_file, delimiter=\",\")\n",
    "    for row in data:\n",
    "        item = corpus_dict.get(row[\"doc_id\"], dict())\n",
    "        item[row[\"word_id\"]] = int(row[\"count\"])\n",
    "        corpus_dict[row[\"doc_id\"]] = item\n",
    "#Get the corpus dict whenever this is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIQUE TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 500 unique topics for the experiment. \n",
    "mi = Mass2Motif.objects.filter(experiment=experiment)\n",
    "unique_topics = {}\n",
    "index=0\n",
    "for m in mi: \n",
    "    unique_topics.update({m.id:index})\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the alphas from the database (it is a vector)\n",
    "al = Alpha.objects.filter(mass2motif__experiment=experiment).order_by('mass2motif')\n",
    "alphas = {}\n",
    "for a in al:\n",
    "    alphas.update({unique_topics[a.mass2motif_id]: a.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = alphas.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vector = np.array(alpha_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to text if necessary \n",
    "# np.savetxt(\"alpha.csv\", alpha_vector, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get beta from the database (it is a topic * words 2d matrix - each cell is a probability)\n",
    "beta_pre_pivot = []\n",
    "mi = Mass2MotifInstance.objects.filter(mass2motif__experiment=experiment)\n",
    "for m in mi:\n",
    "    beta_pre_pivot.append([unique_topics[m.mass2motif_id], unique_words[m.feature_id], m.probability]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some topics may have 0 words - these have been reincluded \n",
    "# Creating array from the beta data and subsequently creating a pivot (matrix)\n",
    "output_arr_beta = np.array(beta_pre_pivot)\n",
    "K = len(unique_topics)\n",
    "W = len(unique_words)\n",
    "pivot_table = np.zeros((K, W)).astype('float')\n",
    "i = 0\n",
    "max = len(beta_pre_pivot)\n",
    "while i<max:\n",
    "    pivot_table[int(output_arr_beta[i][0]),int(output_arr_beta[i][1])]=output_arr_beta[i][2]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to get beta csv. \n",
    "# np.savetxt(\"beta.csv\", pivot_table, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the beta pivot table matrix. \n",
    "pivot_table_normalised = pivot_table\n",
    "i = 0\n",
    "while i<500: \n",
    "    row = pivot_table_normalised[i, :]\n",
    "    adjusted_row = row + 1e-8\n",
    "    normalised_row = adjusted_row / np.sum(adjusted_row)\n",
    "    np.sum(normalised_row)\n",
    "    pivot_table_normalised[i, :] = normalised_row\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to output a csv for the beta matrix if needed. \n",
    "# np.savetxt(\"beta_matrix.csv\", pivot_table_normalised, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for visualisation if necessary \n",
    "# my_dpi=150\n",
    "# plt.figure(figsize=(2000/my_dpi, 2000/my_dpi), dpi=my_dpi)\n",
    "# plt.imshow(pivot_table_normalised, aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ORIGINAL THETA(NORM GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the original theta from the database for subsequent comparison \n",
    "# remember theta is just normalised gamma and represents a docs * topics matrix \n",
    "theta = DocumentMass2Motif.objects.filter(document_id__in=experiment_docs)\n",
    "output_data_theta = []\n",
    "for t in theta:\n",
    "    output_data_theta.append([unique_docs[int(t.document_id)], unique_topics[int(t.mass2motif_id)], t.probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 94, 0.902611552679438],\n",
       " [0, 286, 0.0340671553770332],\n",
       " [0, 200, 0.0186782260549867],\n",
       " [0, 414, 0.0109110112103501],\n",
       " [0, 499, 0.0158815907979228]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ORIGINAL PHI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the features related to the instances\n",
    "feature_instance = FeatureInstance.objects.filter(document_id__in=experiment_docs)\n",
    "feature_instance_join = {}\n",
    "for i in feature_instance:\n",
    "    feature_instance_join.update({int(i.id):[int(i.document_id), int(unique_words[i.feature_id])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect docs, topics and features into a list of lists\n",
    "feature_m2m_instance = FeatureMass2MotifInstance.objects.filter(mass2motif__experiment=experiment)\n",
    "phi_list = []\n",
    "for i in feature_m2m_instance:\n",
    "    if i.featureinstance_id in feature_instance_join.keys():\n",
    "        phi_list.append([feature_instance_join[int(i.featureinstance_id)][0], unique_topics[int(i.mass2motif_id)], feature_instance_join[int(i.featureinstance_id)][1],i.probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives the original phi, which in abstract terms is a 3D matrix -> docs * topics * words\n",
    "phi_original = []\n",
    "for line in phi_list: \n",
    "    phi_original.append([line[0],line[2],line[1],line[3]])\n",
    "phi_original_array = np.array(phi_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"phi_original_array.csv\", phi_original_array, delimiter=\",\", fmt=\"%s\")\n",
    "# np.save(\"phi_original_array\", phi_original_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-STEP (has 9 steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - E-step variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_vector is already mentioned above\n",
    "# beta_matrix is created here from pivot_table_normalised\n",
    "# K and W are from above for total unique topics, total unique words respectively \n",
    "# you need a corpus (created above)\n",
    "corpus = corpus_dict\n",
    "beta_matrix = pivot_table_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Initialise phi matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the 3D matrix phi with zeroes\n",
    "phi_matrix={}\n",
    "for doc in corpus: \n",
    "    d = int(doc)\n",
    "    phi_matrix[d] = {}\n",
    "    for word in corpus[doc]:\n",
    "        w = int(word)\n",
    "        phi_matrix[d][w]=np.zeros(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - initialise gamma matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gamma matrix with rows as documents and columns as topics \n",
    "# this will later be transposed in order to create the phi matrix in the steps 3-9 below\n",
    "# doc_total is the count of words per doc, and each gamma is alpha plus that\n",
    "\n",
    "gamma_matrix=np.zeros((int(len(corpus)),int(K))) #3x500 shape\n",
    "for doc in corpus:\n",
    "    doc_total=0.0\n",
    "    for word in corpus[doc]:\n",
    "        doc_total += corpus[doc][word]\n",
    "    gamma_matrix[int(doc),:] = alpha_vector + 1.0*(doc_total/K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732, 14.732,\n",
       "        14.732, 14.732, 14.732, 14.732]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - 9: repeat until convergence loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "55511.27194821689\n",
      "1\n",
      "56105.65802165169\n",
      "2\n",
      "56700.04409508646\n",
      "3\n",
      "57294.43016852122\n",
      "4\n",
      "57888.816241955996\n",
      "5\n",
      "58483.202315390794\n",
      "6\n",
      "59077.58838882556\n",
      "7\n",
      "59671.97446226033\n",
      "8\n",
      "60266.3605356951\n",
      "9\n",
      "60860.7466091299\n"
     ]
    }
   ],
   "source": [
    "# initialise phi and do Blei's loop \n",
    "n_words = int(len(unique_words))\n",
    "temp_beta = np.zeros((K, n_words))\n",
    "current_gamma = gamma_matrix\n",
    "for i in range(10):   \n",
    "    prev_gamma = current_gamma\n",
    "    for doc in corpus:\n",
    "        d = int(doc)\n",
    "        doc_dict = corpus[doc]\n",
    "        temp_gamma = np.zeros(K) + alpha_vector\n",
    "        for word in doc_dict: #the word is actually column positioning so we do not need n^3 complexity \n",
    "            w = int(word)\n",
    "            log_phi_matrix = np.log(beta_matrix[:,w]) + psi(gamma_matrix[d,:]).T\n",
    "            log_phi_matrix = np.exp(log_phi_matrix - log_phi_matrix.max())\n",
    "            phi_matrix[d][w] = log_phi_matrix/log_phi_matrix.sum()\n",
    "            temp_gamma += phi_matrix[d][w]*corpus[doc][word]\n",
    "            temp_beta[:,w] += phi_matrix[d][w] * corpus[doc][word]\n",
    "        gamma_matrix[d,:] += temp_gamma[0,]\n",
    "        pos = np.where(gamma_matrix[d,:]<SMALL_NUMBER)[0]\n",
    "        gamma_matrix[d,pos] = SMALL_NUMBER\n",
    "    current_gamma = gamma_matrix\n",
    "    gamma_diff = ((current_gamma - prev_gamma)**2).sum()\n",
    "#     beta_matrix = temp_beta\n",
    "    print(i)\n",
    "    print(gamma_matrix.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19365850e-06, 7.15192127e-01, 1.27815026e-06, 1.18438250e-06,\n",
       "       1.15722327e-06, 1.19413969e-06, 1.14302167e-06, 1.24730235e-06,\n",
       "       1.20425179e-06, 1.29807873e-06, 1.25402051e-06, 1.22122806e-06,\n",
       "       1.20743707e-06, 1.36273091e-06, 1.28684497e-06, 1.38617915e-06,\n",
       "       1.21318732e-06, 1.17359787e-06, 1.62376000e-01, 1.22702012e-06,\n",
       "       1.13064617e-06, 1.21655649e-06, 1.30498855e-06, 1.23680168e-06,\n",
       "       1.65296936e-06, 3.92740357e-01, 1.62761222e-01, 1.17924176e-06,\n",
       "       1.47230043e-06, 1.14763635e-06, 1.22349590e-06, 1.24557351e-06,\n",
       "       1.27846424e-06, 5.96719681e-01, 1.23555066e-06, 1.21639975e-06,\n",
       "       1.18197511e-06, 1.21753982e-06, 1.23233747e-06, 1.31651875e-06,\n",
       "       1.41202588e-06, 1.15294789e-06, 1.15618268e-06, 1.21598682e-06,\n",
       "       1.28912583e-06, 1.35785931e-06, 1.18804881e-06, 1.28041389e-06,\n",
       "       1.18722998e-06, 1.29774883e-06, 1.31427024e-06, 2.18019747e+00,\n",
       "       1.14315596e-06, 1.51250111e-06, 1.01820944e-06, 1.30542041e-06,\n",
       "       1.32550359e-06, 1.22469153e-06, 1.23690699e-06, 1.13985746e-06,\n",
       "       1.20229730e-06, 1.26190830e-06, 1.33894438e-06, 1.39042891e-06,\n",
       "       1.41147440e-06, 2.72901531e-01, 1.34603827e-06, 1.19354865e-06,\n",
       "       1.41942724e-06, 2.10979323e-01, 1.13592563e-06, 1.23756140e-06,\n",
       "       1.12863936e-06, 8.16897240e-01, 1.27034006e-06, 2.27871160e-01,\n",
       "       1.18024694e-06, 1.24012235e-06, 1.16719857e-06, 1.21814352e-06,\n",
       "       1.18485673e-06, 1.13375976e-06, 1.39122645e-06, 1.36485646e-06,\n",
       "       1.15803912e-06, 1.45655510e-01, 1.29287209e-06, 1.62437122e-01,\n",
       "       1.22716294e-06, 1.33354456e-06, 1.30436915e-06, 1.21775008e-06,\n",
       "       7.28709002e-01, 2.18201481e-01, 1.29451039e-06, 1.22459478e-06,\n",
       "       1.21525306e-06, 1.27060575e-06, 1.15170506e-06, 1.20256194e-06,\n",
       "       1.39524945e-06, 1.29549065e-06, 1.23875325e-06, 1.21970414e-06,\n",
       "       1.27902784e-06, 1.49305339e-01, 1.25431393e-06, 1.21506264e-06,\n",
       "       1.55447297e-06, 1.22497760e-06, 1.37056294e-06, 1.25875900e-06,\n",
       "       4.92261473e-01, 1.16259795e-06, 1.00638268e+00, 1.22498084e-06,\n",
       "       1.19269357e-06, 1.22827203e-06, 1.16672065e-06, 1.16728331e-06,\n",
       "       1.30258790e-06, 1.37180960e-06, 1.15515050e-06, 1.19169336e-06,\n",
       "       1.23270187e-06, 1.20319685e-06, 6.02035411e-01, 1.22808520e-06,\n",
       "       1.17091168e-06, 1.24936999e-06, 1.28350434e-06, 2.84943773e-01,\n",
       "       1.17703387e-06, 1.24558553e-06, 1.19072438e-06, 7.48609782e-01,\n",
       "       1.15105851e-06, 1.28980141e-06, 1.31473279e-06, 1.56517878e-06,\n",
       "       1.21086470e-06, 3.10311984e-01, 1.15771903e-06, 1.32595596e-06,\n",
       "       3.35318018e-01, 1.26103299e-06, 1.23431054e-06, 1.11663848e+00,\n",
       "       1.19363736e-06, 1.31856622e-06, 1.16990481e-06, 3.89418027e-01,\n",
       "       7.02416887e-01, 2.29862488e-06, 8.13574101e-01, 1.20683526e-06,\n",
       "       1.23953354e-06, 1.18115519e-06, 1.42292052e-01, 3.64750835e-03,\n",
       "       1.22189819e-06, 1.16336662e-06, 1.25575319e-06, 1.23378264e-06,\n",
       "       1.16954145e-06, 1.25761166e-06, 1.21788855e-06, 1.17851361e-06,\n",
       "       1.28913366e-06, 1.25008664e-06, 1.28321696e-06, 1.31483445e-06,\n",
       "       1.36186499e-06, 1.27294861e-06, 1.29006479e-06, 1.24595997e-06,\n",
       "       1.22819911e-06, 1.23811856e-06, 1.20970124e-06, 1.14160975e-06,\n",
       "       2.78976269e-01, 1.18159012e-06, 1.18408189e-06, 1.25045636e-06,\n",
       "       1.17528983e-06, 1.19228088e-06, 1.10310251e-06, 1.19918639e-06,\n",
       "       1.31889135e-06, 1.32594191e-06, 1.14209493e-06, 1.16157313e-06,\n",
       "       1.16944495e-06, 1.36053980e-01, 1.30203976e-06, 1.21566361e-06,\n",
       "       1.16378883e-06, 1.59669650e-06, 1.17754210e-06, 1.17876354e-06,\n",
       "       1.15474940e-06, 1.28025082e-06, 1.23236917e-06, 3.10695130e-01,\n",
       "       1.18732275e-06, 1.16674055e-06, 1.21427811e-06, 1.23317202e-06,\n",
       "       1.21574508e-06, 1.19435493e-06, 1.27631744e-06, 1.17847322e-06,\n",
       "       1.20338699e-06, 1.25546135e-06, 1.21197069e-06, 1.23322080e-06,\n",
       "       1.18294897e-06, 1.21270448e-06, 1.21562431e-06, 1.32702246e-06,\n",
       "       1.30883920e-06, 2.92224451e-01, 1.23240603e-06, 1.24164163e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.01820944e-06, 1.15015107e-06,\n",
       "       1.36511762e-06, 1.25797541e-06, 1.21029122e-06, 1.21904889e-06,\n",
       "       1.25182705e-06, 1.41615742e-06, 2.02642784e-01, 1.26488484e-06,\n",
       "       1.24676807e-06, 1.01820944e-06, 1.19091610e-06, 1.20339932e-06,\n",
       "       1.13790488e-06, 1.36490179e-06, 1.16235835e-06, 1.41011321e-06,\n",
       "       3.90898966e-01, 1.18651639e-06, 1.06624972e-06, 1.31023091e-06,\n",
       "       1.14675424e-06, 1.25453675e-06, 1.25834724e-06, 1.16752849e-06,\n",
       "       1.27554912e-06, 1.31619654e-06, 1.19397459e-06, 1.24390321e-06,\n",
       "       1.52741213e-06, 1.12015502e-06, 1.36965029e-06, 1.20112460e-06,\n",
       "       3.62362867e+00, 1.65528330e-01, 1.34224584e-06, 2.97778169e-01,\n",
       "       1.14405233e-06, 1.25261251e-06, 1.19502556e-06, 1.22019739e-06,\n",
       "       3.68367009e-01, 3.16832639e-01, 1.21057277e-06, 1.59175172e-06,\n",
       "       1.27445285e-06, 1.21754624e-06, 1.16456915e-06, 1.16099100e-06,\n",
       "       2.61285596e-01, 1.17369013e-06, 1.15600858e-06, 1.32482034e-06,\n",
       "       1.19918152e-06, 2.71027352e-01, 1.44174549e-06, 1.16473371e-06,\n",
       "       1.21687366e-06, 1.22397258e-06, 1.13402194e-06, 1.23588279e-06,\n",
       "       1.34387952e-06, 1.52874013e-06, 1.30035322e-06, 1.22631051e-06,\n",
       "       1.26220518e-06, 1.22974993e-06, 1.68330752e-06, 1.42827604e-06,\n",
       "       1.16247781e-06, 1.15029597e-06, 1.13250010e-06, 1.21169384e-06,\n",
       "       1.36304846e-06, 2.71020651e-01, 1.22527179e-06, 1.32597549e-06,\n",
       "       1.11303183e-06, 1.19667640e-06, 1.20467033e-06, 1.17338743e-06,\n",
       "       1.14656058e-06, 1.19489338e-06, 1.24084891e-06, 1.17918516e-06,\n",
       "       1.34397991e-06, 1.25258470e-06, 1.22231000e-06, 1.13719079e-06,\n",
       "       1.27350798e-06, 1.27691091e-06, 1.20308538e-06, 1.24468684e-06,\n",
       "       1.25878213e-06, 1.18185767e-06, 1.21036840e-06, 1.47590461e-06,\n",
       "       1.17460189e-06, 1.25337894e-06, 1.17520531e-06, 1.28604003e-06,\n",
       "       1.24210351e-06, 1.19171200e-06, 1.14481396e-06, 1.49075652e-06,\n",
       "       1.05150312e-06, 1.27165659e-06, 1.17497303e-06, 1.28497079e-06,\n",
       "       1.24070945e-06, 1.21842643e-06, 1.20018772e-06, 1.26845411e-06,\n",
       "       1.18974910e-06, 1.23779987e-06, 1.27022057e-06, 1.24851440e-06,\n",
       "       1.42758197e-06, 1.21669100e-06, 1.15801104e-06, 1.30492559e-06,\n",
       "       1.20390122e-06, 1.16261478e-06, 1.47069656e-06, 2.93322694e-01,\n",
       "       1.24721445e-06, 1.21899631e-06, 1.29185492e-06, 3.73366992e-01,\n",
       "       1.18033132e-06, 1.13388463e-06, 5.41806649e-01, 4.16962908e-01,\n",
       "       1.49449633e-01, 2.87085722e-01, 1.10928096e-06, 1.30487409e-06,\n",
       "       1.17276926e-06, 1.43972760e-01, 1.15964397e-06, 1.18287409e-06,\n",
       "       1.26775892e-06, 1.33311809e-06, 1.40984802e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.01820944e-06, 1.13764668e-06,\n",
       "       1.26942528e-06, 1.22907685e-06, 1.17234017e-06, 1.19383915e-06,\n",
       "       1.25757219e-06, 1.32720388e-06, 4.56315569e-01, 1.10411496e-06,\n",
       "       1.23664901e-06, 1.37568737e-06, 1.19014215e-06, 1.15567346e-06,\n",
       "       1.24911857e-06, 1.13502836e-06, 1.28877449e-06, 1.25083736e-06,\n",
       "       1.16794915e-06, 1.27362727e-06, 3.82594466e-01, 1.19213475e-06,\n",
       "       1.16826275e-06, 1.13602478e-06, 2.67115442e-01, 1.27322767e-06,\n",
       "       1.18509901e-06, 1.18136326e-06, 1.01820944e-06, 1.25634378e-06,\n",
       "       1.14047737e-06, 1.14335162e-06, 1.14767559e-06, 1.17462167e-06,\n",
       "       1.27544535e-06, 1.17732914e-06, 1.18951137e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.01820944e-06, 1.34291823e-06,\n",
       "       1.28166364e-06, 1.12547353e-06, 3.97342664e-01, 1.18386513e-06,\n",
       "       1.62160821e+00, 1.11331011e-06, 1.25851804e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.77971649e-01, 7.44901889e-01,\n",
       "       1.19832589e-06, 1.10330600e-06, 1.13788444e-06, 1.12408942e-06,\n",
       "       1.16875725e-06, 1.35285351e-06, 1.18585822e-06, 1.20343589e-06,\n",
       "       1.12817582e-06, 1.14907706e-06, 1.35134306e-06, 1.21644702e-06,\n",
       "       1.20068080e-06, 3.65813802e-01, 1.39888814e-06, 3.61001352e-01,\n",
       "       1.21318475e-06, 1.17845030e-06, 1.28377622e-06, 1.20878050e-06,\n",
       "       1.22738668e-06, 1.01820944e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.58980264e-06, 1.18672576e-06, 1.40115765e-06, 1.28953782e-06,\n",
       "       3.83036980e-01, 1.23518670e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.01820944e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.01820944e-06, 1.77437536e-01, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.26431675e-06, 1.01820944e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.13926563e-06, 1.01820944e-06, 1.01820944e-06, 1.01820944e-06,\n",
       "       1.17450490e-06, 1.17665189e-06, 1.12722941e-06, 1.14544894e-06,\n",
       "       1.24684729e-06, 1.19539199e-06, 1.24303171e-06, 1.16960758e-06,\n",
       "       1.13658707e-06, 1.15971753e-06, 1.13863839e-06, 1.16276788e-06,\n",
       "       1.13776752e-06, 1.13599626e-06, 8.77250702e-01, 1.43033532e+01])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lola = phi_matrix[0][10001]*corpus['0']['10001']\n",
    "lola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON (phi gamma original vs calculated) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma comparison / actually Theta comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check phi_matrix vs phi_list \n",
    "#first bring to same format\n",
    "\n",
    "phi_calculated = []\n",
    "for line in phi_list: \n",
    "    phi_calculated.append([line[0],line[2],line[1],phi_matrix[line[0]][line[2]][line[1]]])\n",
    "\n",
    "phi_calculated_array = np.array(phi_calculated)\n",
    "np.savetxt(\"phi_calculated_array.csv\", phi_calculated_array, delimiter=\",\", fmt=\"%s\")          \n",
    "np.save(\"phi_calculated_array\", phi_calculated_array)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
